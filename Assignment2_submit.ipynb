{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP5046 Assignment2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def read_data(file_name):\n",
    "    df = pd.read_csv(file_name)\n",
    "    Sentences = ' '.join(df.Sentence.tolist()).split(' . ')\n",
    "    word = ' '.join(df.Sentence.tolist()).split(' ')\n",
    "    try:\n",
    "        target = ' '.join(df.NER.tolist()).split(' ')\n",
    "\n",
    "    except:\n",
    "        target = df.NER.tolist()\n",
    "    train = [sentence.strip().split() + ['.'] for sentence in Sentences]\n",
    "    train[-1] = train[-1][:-1]\n",
    "    i = 0\n",
    "    tags = []\n",
    "    for sentence in train:\n",
    "        tag = target[i:i + len(sentence)]\n",
    "        tags.append(tag)\n",
    "        i += len(sentence)\n",
    "    print(\"data has been loaded!\")\n",
    "    return train, tags\n",
    "\n",
    "\n",
    "def read_test_data(file_name):\n",
    "    df = pd.read_csv(file_name)\n",
    "    Sentences = ' '.join(df.Sentence.tolist()).split(' . ')\n",
    "    word = ' '.join(df.Sentence.tolist()).split(' ')\n",
    "    train = [sentence.strip().split() + ['.'] for sentence in Sentences]\n",
    "    train[-1] = train[-1][:-1]\n",
    "    print(\"test data has been loaded!\")\n",
    "    return train\n",
    "\n",
    "\n",
    "train_data, target_y_train = read_data(\"train.csv\")\n",
    "validation_data, target_y_validation = read_data(\"val.csv\")\n",
    "test_data = read_test_data('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Build Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 TF-IDF Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(data):\n",
    "    import numpy as np\n",
    "    DF = {}\n",
    "\n",
    "    for tokensized_doc in data:\n",
    "        # get each unique word in the doc - we need to know whether the word is appeared in the document\n",
    "        for term in np.unique(tokensized_doc):\n",
    "            try:\n",
    "                DF[term] += 1\n",
    "            except:\n",
    "                DF[term] = 1\n",
    "\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    from collections import Counter\n",
    "    import math\n",
    "\n",
    "    tf_idf = {}\n",
    "\n",
    "    # total number of documents\n",
    "    N = len(data)\n",
    "\n",
    "    doc_id = 0\n",
    "    # get each tokenised doc\n",
    "    for tokensized_doc in data:\n",
    "        # initialise counter for the doc\n",
    "        counter = Counter(tokensized_doc)\n",
    "        # calculate total number of words in the doc\n",
    "        total_num_words = len(tokensized_doc)\n",
    "\n",
    "        # get each unique word in the doc\n",
    "        for term in np.unique(tokensized_doc):\n",
    "            # calculate Term Frequency\n",
    "            tf = counter[term] / total_num_words\n",
    "\n",
    "            # calculate Document Frequency\n",
    "            df = DF[term]\n",
    "\n",
    "            # calculate Inverse Document Frequency\n",
    "            idf = math.log(N / (df + 1)) + 1\n",
    "\n",
    "            # calculate TF-IDF\n",
    "            tf_idf[doc_id, term] = tf * idf\n",
    "\n",
    "        doc_id += 1\n",
    "\n",
    "    TF_doc=[]\n",
    "    for i in range(N):\n",
    "        temp=[]\n",
    "        for word in data[i]:\n",
    "            temp.append(tf_idf[(i,word)])\n",
    "        TF_doc.append(temp)\n",
    "\n",
    "    return TF_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 POS Tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Build Feature to Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1 Build Word and Tag Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_ix = {}\n",
    "for sentence in train_data+validation_data+test_data:\n",
    "    for word in sentence:\n",
    "        word = word.lower()\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "word_list = list(word_to_ix.keys())\n",
    "\n",
    "START_TAG = \"<START>\"\n",
    "STOP_TAG = \"<STOP>\"\n",
    "tag_to_ix = {START_TAG:0, STOP_TAG:1}\n",
    "for tags in target_y_train+target_y_validation:\n",
    "    for tag in tags:\n",
    "        if tag not in tag_to_ix:\n",
    "            tag_to_ix[tag] = len(tag_to_ix)\n",
    "\n",
    "pos_to_ix = {START_TAG:0, STOP_TAG:1}\n",
    "for tags in POS_tag:\n",
    "    for tag in tags:\n",
    "        if tag not in pos_to_ix:\n",
    "            pos_to_ix[tag] = len(pos_to_ix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2 Load Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "word_emb_model = api.load(\"glove-twitter-50\")\n",
    "print(\"=\"*89)\n",
    "print(\"pre-trained word embedding model has been loaded!\")\n",
    "print(\"=\"*89)\n",
    "EMBEDDING_DIM = 50\n",
    "\n",
    "embedding_matrix = []\n",
    "for word in word_list:\n",
    "    try:\n",
    "        embedding_matrix.append(word_emb_model.wv[word])\n",
    "    except:\n",
    "        embedding_matrix.append([0]*EMBEDDING_DIM)\n",
    "embedding_matrix = np.array(embedding_matrix)\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.3 Transform Data to Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_index(data, to_ix):\n",
    "    input_index_list = []\n",
    "    for sent in data:\n",
    "        input_index_list.append([to_ix[w] for w in sent])\n",
    "    return input_index_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos = POS_tag[:len(train_data)]\n",
    "val_pos = POS_tag[len(train_data):(len(train_data+validation_data))]\n",
    "test_pos = POS_tag[-len(test_data):]\n",
    "\n",
    "train_input_index =  to_index(train_data,word_to_ix)\n",
    "train_pos_index =  to_index(train_pos,pos_to_ix)\n",
    "train_output_index = to_index(target_y_train,tag_to_ix)\n",
    "val_input_index = to_index(validation_data,word_to_ix)\n",
    "val_pos_index =  to_index(val_pos,pos_to_ix)\n",
    "val_output_index = to_index(target_y_validation,tag_to_ix)\n",
    "test_input_index = to_index(test_data,word_to_ix)\n",
    "test_pos_index =  to_index(test_pos,pos_to_ix)\n",
    "\n",
    "train_tf_idf_index = tf_idf(train_data)\n",
    "val_tf_idf_index = tf_idf(validation_data)\n",
    "test_tf_idf_index = tf_idf(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Model Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Build NER Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def argmax(vec):\n",
    "    # return the argmax as a python int\n",
    "    _, idx = torch.max(vec, 1)\n",
    "    return idx.item()\n",
    "\n",
    "\n",
    "# Compute log sum exp in a numerically stable way for the forward algorithm\n",
    "def log_sum_exp(vec):\n",
    "    max_score = vec[0, argmax(vec)]\n",
    "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
    "    return max_score + \\\n",
    "           torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "\n",
    "def argmax(vec):\n",
    "    # return the argmax as a python int\n",
    "    _, idx = torch.max(vec, 1)\n",
    "    return idx.item()\n",
    "\n",
    "\n",
    "# Compute log sum exp in a numerically stable way for the forward algorithm\n",
    "def log_sum_exp(vec):\n",
    "    max_score = vec[0, argmax(vec)]\n",
    "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
    "    return max_score + \\\n",
    "           torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))\n",
    "\n",
    "\n",
    "class BiLSTM_CRF(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, lstm_dim=90, embedding_dim=50, hidden_dim=128, num_layers=2,\n",
    "                 method='ATTN_TYPE_DOT_PRODUCT'):\n",
    "        super(BiLSTM_CRF, self).__init__()\n",
    "        self.method = method\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.tag_to_ix = tag_to_ix\n",
    "        self.tagset_size = len(tag_to_ix)\n",
    "        self.pos_to_ix = pos_to_ix\n",
    "        self.posset_size = len(pos_to_ix)\n",
    "        self.lstm_dim = lstm_dim\n",
    "        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        \"\"\"Here we use the embedding matrix as the ,initial weights of nn.Embedding\"\"\"\n",
    "        self.word_embeds.weight.data.copy_(torch.from_numpy(embedding_matrix))\n",
    "\n",
    "        self.lstm = nn.LSTM(lstm_dim, hidden_dim // 2,\n",
    "                            num_layers=self.num_layers, bidirectional=True)\n",
    "\n",
    "        # Maps the output of the LSTM into tag space.\n",
    "        self.hidden2tag = nn.Linear(hidden_dim * 2, self.tagset_size)\n",
    "\n",
    "        # Matrix of transition parameters.  Entry i,j is the score of\n",
    "        # transitioning *to* i *from* j.\n",
    "        self.transitions = nn.Parameter(\n",
    "            torch.randn(self.tagset_size, self.tagset_size))\n",
    "\n",
    "        # These two statements enforce the constraint that we never transfer\n",
    "        # to the start tag and we never transfer from the stop tag\n",
    "        self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n",
    "        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n",
    "\n",
    "        self.hidden = self.init_hidden()\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return (torch.randn(2 * self.num_layers, 1, self.hidden_dim // 2).to(device),\n",
    "                torch.randn(2 * self.num_layers, 1, self.hidden_dim // 2).to(device))\n",
    "\n",
    "    def _forward_alg(self, feats):\n",
    "        # Do the forward algorithm to compute the partition function\n",
    "        init_alphas = torch.full((1, self.tagset_size), -10000.).to(device)\n",
    "        # START_TAG has all of the score.\n",
    "        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n",
    "\n",
    "        # Wrap in a variable so that we will get automatic backprop\n",
    "        forward_var = init_alphas\n",
    "\n",
    "        # Iterate through the sentence\n",
    "        for feat in feats:\n",
    "            alphas_t = []  # The forward tensors at this timestep\n",
    "            for next_tag in range(self.tagset_size):\n",
    "                # broadcast the emission score: it is the same regardless of\n",
    "                # the previous tag\n",
    "                emit_score = feat[next_tag].view(\n",
    "                    1, -1).expand(1, self.tagset_size)\n",
    "                # the ith entry of trans_score is the score of transitioning to\n",
    "                # next_tag from i\n",
    "                trans_score = self.transitions[next_tag].view(1, -1)\n",
    "                # The ith entry of next_tag_var is the value for the\n",
    "                # edge (i -> next_tag) before we do log-sum-exp\n",
    "                next_tag_var = forward_var + trans_score + emit_score\n",
    "                # The forward variable for this tag is log-sum-exp of all the\n",
    "                # scores.\n",
    "                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n",
    "            forward_var = torch.cat(alphas_t).view(1, -1)\n",
    "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
    "        alpha = log_sum_exp(terminal_var)\n",
    "        return alpha\n",
    "\n",
    "    def _get_lstm_features(self, sentence, pos_tagging=None, tf_idf=None):\n",
    "        self.hidden = self.init_hidden()\n",
    "        embeds = self.word_embeds(sentence).view(len(sentence), -1)\n",
    "        combined = embeds.unsqueeze(dim=1)\n",
    "        if pos_tagging is not None:\n",
    "            pos = torch.eye(self.posset_size).to(device)[pos_tagging].unsqueeze(dim=1)\n",
    "            combined = torch.cat((combined, pos), 2)\n",
    "        if tf_idf is not None:\n",
    "            tf_idf = torch.tensor(np.array(tf_idf), dtype=torch.float).unsqueeze(1).unsqueeze(dim=1)\n",
    "            combined = torch.cat((combined, tf_idf), 2)\n",
    "        lstm_out, self.hidden = self.lstm(combined, self.hidden)\n",
    "        lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\n",
    "\n",
    "        return lstm_out\n",
    "\n",
    "    def _cal_attention(self, lstm_out, method):\n",
    "        attention_result = torch.zeros(lstm_out.size()[0], self.hidden_dim * 2, device=device)\n",
    "        if method == 'ATTN_TYPE_DOT_PRODUCT':\n",
    "            # bmm: https://pytorch.org/docs/master/generated/torch.bmm.html\n",
    "            for i in range(lstm_out.size()[0]):\n",
    "                hidden = lstm_out[i]\n",
    "                attn_weights = F.softmax(torch.bmm(hidden.unsqueeze(0).unsqueeze(0), lstm_out.T.unsqueeze(0)), dim=-1)\n",
    "                attn_output = torch.bmm(attn_weights, lstm_out.unsqueeze(0))\n",
    "                concat_output = torch.cat((hidden.unsqueeze(0), attn_output[0]), 1)\n",
    "                attention_result[i] = concat_output.squeeze(0)\n",
    "        elif method == 'ATTN_TYPE_SCALE_DOT_PRODUCT':\n",
    "            for i in range(lstm_out.size()[0]):\n",
    "                hidden = lstm_out[i]\n",
    "                attn_weights = F.softmax(\n",
    "                    1 / np.sqrt(self.hidden_dim) * torch.bmm(hidden.unsqueeze(0).unsqueeze(0), lstm_out.T.unsqueeze(0)),\n",
    "                    dim=-1)\n",
    "                attn_output = torch.bmm(attn_weights, lstm_out.unsqueeze(0))\n",
    "                concat_output = torch.cat((hidden.unsqueeze(0), attn_output[0]), 1)\n",
    "                attention_result[i] = concat_output.squeeze(0)\n",
    "        attention_out = self.hidden2tag(attention_result)\n",
    "        return attention_out\n",
    "\n",
    "    def _score_sentence(self, feats, tags):\n",
    "        # Gives the score of a provided tag sequence\n",
    "        score = torch.zeros(1).to(device)\n",
    "        tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long).to(device), tags])\n",
    "        for i, feat in enumerate(feats):\n",
    "            score = score + \\\n",
    "                    self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
    "        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n",
    "        return score\n",
    "\n",
    "    def _viterbi_decode(self, feats):\n",
    "        backpointers = []\n",
    "\n",
    "        # Initialize the viterbi variables in log space\n",
    "        init_vvars = torch.full((1, self.tagset_size), -10000.).to(device)\n",
    "        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n",
    "\n",
    "        # forward_var at step i holds the viterbi variables for step i-1\n",
    "        forward_var = init_vvars\n",
    "        for feat in feats:\n",
    "            bptrs_t = []  # holds the backpointers for this step\n",
    "            viterbivars_t = []  # holds the viterbi variables for this step\n",
    "\n",
    "            for next_tag in range(self.tagset_size):\n",
    "                # next_tag_var[i] holds the viterbi variable for tag i at the\n",
    "                # previous step, plus the score of transitioning\n",
    "                # from tag i to next_tag.\n",
    "                # We don't include the emission scores here because the max\n",
    "                # does not depend on them (we add them in below)\n",
    "                next_tag_var = forward_var + self.transitions[next_tag]\n",
    "                best_tag_id = argmax(next_tag_var)\n",
    "                bptrs_t.append(best_tag_id)\n",
    "                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
    "            # Now add in the emission scores, and assign forward_var to the set\n",
    "            # of viterbi variables we just computed\n",
    "            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
    "            backpointers.append(bptrs_t)\n",
    "\n",
    "        # Transition to STOP_TAG\n",
    "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
    "        best_tag_id = argmax(terminal_var)\n",
    "        path_score = terminal_var[0][best_tag_id]\n",
    "\n",
    "        # Follow the back pointers to decode the best path.\n",
    "        best_path = [best_tag_id]\n",
    "        for bptrs_t in reversed(backpointers):\n",
    "            best_tag_id = bptrs_t[best_tag_id]\n",
    "            best_path.append(best_tag_id)\n",
    "        # Pop off the start tag (we dont want to return that to the caller)\n",
    "        start = best_path.pop()\n",
    "        assert start == self.tag_to_ix[START_TAG]  # Sanity check\n",
    "        best_path.reverse()\n",
    "        return path_score, best_path\n",
    "\n",
    "    def neg_log_likelihood(self, sentence, tags, pos_tagging=None, tf_idf=None):\n",
    "        lstm_out = self._get_lstm_features(sentence, pos_tagging, tf_idf)\n",
    "\n",
    "        attention_feats = self._cal_attention(lstm_out, self.method)\n",
    "        forward_score = self._forward_alg(attention_feats)\n",
    "        gold_score = self._score_sentence(attention_feats, tags)\n",
    "        return forward_score - gold_score\n",
    "\n",
    "    def forward(self, sentence, pos_tagging=None, tf_idf=None):  # dont confuse this with _forward_alg above.\n",
    "        # Get the emission scores from the BiLSTM\n",
    "        lstm_out = self._get_lstm_features(sentence, pos_tagging, tf_idf)\n",
    "\n",
    "        attention_feats = self._cal_attention(lstm_out, self.method)\n",
    "\n",
    "        # Find the best path, given the features.\n",
    "\n",
    "        score, tag_seq = self._viterbi_decode(attention_feats)\n",
    "        return score, tag_seq, attention_feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Train NER Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_configs = [\n",
    "    {\n",
    "        'Embedding': True,\n",
    "        'Pos': False,\n",
    "        'tf_idf': False,\n",
    "        'lstm_dim': 50,\n",
    "        'accuracy': [],\n",
    "        'f1 score': []\n",
    "    },\n",
    "    {\n",
    "        'Embedding': True,\n",
    "        'Pos': True,\n",
    "        'tf_idf': False,\n",
    "        'lstm_dim': 89,\n",
    "        'accuracy': [],\n",
    "        'f1 score': []\n",
    "    },\n",
    "    {\n",
    "        'Embedding': True,\n",
    "        'Pos': False,\n",
    "        'tf_idf': True,\n",
    "        'lstm_dim': 51,\n",
    "        'accuracy': [],\n",
    "        'f1 score': []\n",
    "    },\n",
    "    {\n",
    "        'Embedding': True,\n",
    "        'Pos': True,\n",
    "        'tf_idf': True,\n",
    "        'lstm_dim': 90,\n",
    "        'accuracy': [],\n",
    "        'f1 score': []\n",
    "    }\n",
    "]\n",
    "\n",
    "attention_configs = [\n",
    "    {\n",
    "        'Embedding': True,\n",
    "        'Pos': True,\n",
    "        'tf_idf': True,\n",
    "        'attention_name': 'ATTN_TYPE_DOT_PRODUCT',\n",
    "        'accuracy': [],\n",
    "        'f1 score': []\n",
    "    },\n",
    "    {\n",
    "        'Embedding': True,\n",
    "        'Pos': True,\n",
    "        'tf_idf': True,\n",
    "        'attention_name': 'ATTN_TYPE_SCALE_DOT_PRODUCT',\n",
    "        'accuracy': [],\n",
    "        'f1 score': []\n",
    "    }\n",
    "]\n",
    "\n",
    "model_structure_configs = [\n",
    "    {\n",
    "        'nlayer': 1,\n",
    "        'accuracy': [],\n",
    "        'f1 score': []\n",
    "    },\n",
    "    {\n",
    "        'nlayer': 2,\n",
    "        'accuracy': [],\n",
    "        'f1 score': []\n",
    "    },\n",
    "    {\n",
    "        'nlayer': 4,\n",
    "        'accuracy': [],\n",
    "        'f1 score': []\n",
    "    },\n",
    "    {\n",
    "        'nlayer': 8,\n",
    "        'accuracy': [],\n",
    "        'f1 score': []\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_iter(model, sentence, target, optimizer, POS_Tagging, tf_idf):\n",
    "    '''\n",
    "    Train the model for a single iteration.\n",
    "    An iteration is when a single batch of data is passed forward and\n",
    "    backward through the neural network.\n",
    "    '''\n",
    "    loss = model.neg_log_likelihood(sentence, target, POS_Tagging, tf_idf)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(log_interval, model, device, optimizer, epoch, POS_Tagging=True, tf_idf=True):\n",
    "    '''\n",
    "    Train the model for an epoch.\n",
    "    An epoch is when the entire dataset is passed forward and\n",
    "    backward through the neural network for once.\n",
    "    The number of batches in a dataset is equal to number of iterations for one epoch.\n",
    "    '''\n",
    "    time1 = datetime.datetime.now()\n",
    "    train_loss = 0\n",
    "    model.train()\n",
    "    for i, idxs in enumerate(train_input_index):\n",
    "        tags_index = train_output_index[i]\n",
    "        if POS_Tagging:\n",
    "            pos_index = train_pos_index[i]\n",
    "            pos_in = torch.tensor(pos_index, dtype=torch.long).to(device)\n",
    "        else:\n",
    "            pos_in = None\n",
    "        if tf_idf:\n",
    "            tf_idf_index = train_tf_idf_index[i]\n",
    "            tf_idf_in = torch.tensor(tf_idf_index, dtype=torch.float).to(device)\n",
    "        else:\n",
    "            tf_idf_in = None\n",
    "\n",
    "        model.zero_grad()\n",
    "        sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n",
    "        targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n",
    "        loss = train_iter(model, sentence=sentence_in, target=targets, optimizer=optimizer, POS_Tagging=pos_in,\n",
    "                          tf_idf=tf_idf_in)\n",
    "        train_loss += loss\n",
    "        time2 = datetime.datetime.now()\n",
    "        if (i + 1) % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tTime: {:.2f}s'.format(\n",
    "                epoch, (i + 1), len(train_input_index), 100. * (i + 1) / len(train_input_index), train_loss / (i + 1),\n",
    "                (time2 - time1).total_seconds()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Save NER Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Evaluate NER Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, input_index, output_index, pos_index=val_pos_index, tf_idf_index=val_tf_idf_index):\n",
    "    ground_truth = []\n",
    "    predicted = []\n",
    "    for i in range(len(input_index)):\n",
    "        input_sent = input_index[i]\n",
    "        input_sent = torch.tensor(input_sent, dtype=torch.long).to(device)\n",
    "        output_tag = output_index[i]\n",
    "        if pos_index is not None:\n",
    "            pos_tag = pos_index[i]\n",
    "            pos_tag = torch.tensor(pos_tag, dtype=torch.long).to(device)\n",
    "        else:\n",
    "            pos_tag = None\n",
    "        if tf_idf_index is not None:\n",
    "            tf_idf = tf_idf_index[i]\n",
    "            tf_idf = torch.tensor(tf_idf, dtype=torch.long).to(device)\n",
    "        else:\n",
    "            tf_idf = None\n",
    "        _, prediction, _ = model(input_sent, pos_tag, tf_idf)\n",
    "        predicted = predicted + prediction\n",
    "        ground_truth = ground_truth + output_tag\n",
    "    accuracy = float((np.array(predicted) == np.array(ground_truth)).astype(int).sum()) / float(len(ground_truth))\n",
    "    f1_ = f1_score(np.array(ground_truth), np.array(predicted),average='micro')\n",
    "    print('Val accuracy: {:.6f}\\tF1 score: {:.2f}'.format(accuracy, f1_),)\n",
    "    return ground_truth, predicted, accuracy, f1_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Performance Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Ablation Study - different embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_EPOCH = 10\n",
    "# Different model evaluation\n",
    "for config in model_configs:\n",
    "    model = BiLSTM_CRF(vocab_size=len(word_to_ix), lstm_dim=config['lstm_dim'])\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)\n",
    "    print(\"=\" * 100)\n",
    "    print(\"Start Training\")\n",
    "    for epoch in range(TOTAL_EPOCH):\n",
    "        train_epoch(500, model=model, device=device, optimizer=optimizer, epoch=epoch, POS_Tagging=config['Pos'],\n",
    "                    tf_idf=config['tf_idf'])\n",
    "        print(\"=\" * 100)\n",
    "        print(\"Test\")\n",
    "        _, _, accuracy, f1_ = test(model, val_input_index, val_output_index,\n",
    "                                   pos_index=val_pos_index if config['Pos'] else None,\n",
    "                                   tf_idf_index=val_tf_idf_index if config['tf_idf'] else None)\n",
    "        config['accuracy'].append(accuracy)\n",
    "        print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Ablation Study - different attention strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for config in attention_configs:\n",
    "    model = BiLSTM_CRF(vocab_size=len(word_to_ix), num_layers=config['attention_name'])\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)\n",
    "    print(\"=\" * 100)\n",
    "    print(\"Start Training\")\n",
    "    for epoch in range(TOTAL_EPOCH):\n",
    "        train_epoch(500, model=model, device=device, optimizer=optimizer, epoch=epoch)\n",
    "        print(\"=\" * 100)\n",
    "        print(\"Test\")\n",
    "        _, _, accuracy, f1_ = test(model, val_input_index, val_output_index)\n",
    "        config['accuracy'].append(accuracy)\n",
    "        print(\"=\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Ablation Study - different layer strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for config in model_structure_configs:\n",
    "    model = BiLSTM_CRF(vocab_size=len(word_to_ix), lstm_dim=config['nlayer'])\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)\n",
    "    print(\"=\" * 100)\n",
    "    print(\"Start Training\")\n",
    "    for epoch in range(TOTAL_EPOCH):\n",
    "        train_epoch(500, model=model, device=device, optimizer=optimizer, epoch=epoch)\n",
    "        print(\"=\" * 100)\n",
    "        print(\"Test\")\n",
    "        _, _, accuracy, f1_ = test(model, val_input_index, val_output_index)\n",
    "        config['accuracy'].append(accuracy)\n",
    "        print(\"=\" * 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
